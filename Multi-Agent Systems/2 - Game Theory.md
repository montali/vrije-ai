#  Game theory

This is one of the fundamental ideas behind MAS. We're first going to look at examples of interesting games, and we'll look at basic concepts for solving these. 

## Games

We all know **games**, used as entertainment or challenge, which though can also be used as **models**. All the models are wrong (in the sense that they're simplifications), but some of them are useful!

Game theory started as an **economic theory**, and in the meantime lots of nobel prizes have been won for it. Game theory provides a good level of abstraction for the study of socio-economic, political and biological phenomena. 

The ingredients are the following: **players** (usually having opponents, either other agents, versions of themselves or even *chance events*), **rules** (certain actions cannot be done, others can, resulting in a *payoff* or *utility*). Everyone wants to **maximize their pay-off**. There are two flavours, one being *non-cooperative* (you have selfish individuals, which are only concerned with their own utility, which may still result in accidental cooperation with self-enforcing agreements) or *cooperative* (they bind contracts for collaboration, counting on them). We can further divide this last category into *non-trasferable utility* (the pay-off for each individual increases) and *transferable utility* (needing a fair way to divide the additional value). The ***Shapley value*** captures this idea of *fair distribution*. 

There are simple ways of representing utility, like matrices, decision trees... 

Generally, **utility** is a **function of the joint action** of players, i.e. a function mapping actions to rewards.

The **ultimatum game** is an interesting example: player A can choose any fraction 0<x<1, and B either accepts (getting 1-x) or rejects (both get 0). This explains rationality vs emotionality: a rational agent always accepts.

Let's look at a few non-trivial examples of games.

### Economy: Cournot's Duopoly model

You have 2 companies making an interchangeable product (AH muesli vs Jumbo muesli). Without knowing the other company's strategy, both need to determine the quantity they need to produce. The unit price depends on the total produced quantity: if there's more in the market, the price goes down. The unit price, therefore, is given by $p(q_1, q_2) = \alpha - \beta (q_1+q_2)$.

Suppose they can produce these at a unit cost of $c_1$ and $c_2$ respectively, which quantity should they produce to maximize the profit?

### Children dividing pie

This illustrates various GT concepts. One cuts the cake, the other one makes the first choice. This is a minimax and maximin problem: the cutter will mitigate the worst result, while the chooser will maximize his pay-off. This is sometimes called *reversed game theory*: you start from the desired output, and you derive what you'll want to lose.

### Prisoner's dilemma

*Two suspects in a crime are put into separate cells. The police officer tells them: Currently you’re charged with trespassing which carries a jail sentence of one month. I know you were planning a robbery though, but cannot prove it – I need your testimony. If you confess and cooperate, I will drop the charges against you, but your partner will be charged to the fullest extent of the law: 12 months in jail. I’m offering the same deal to him. If you both confess, your individual testimony is less valuable, and you will get 8 months each.Penalty kick game*

### Penalty kicks

Studied extensively in sports literature, if the kicker kicks a penalty the goalkeeper doesn't have time and he just has to **guess** the direction. 

### Ice cream time

Suppose you have a long beach, and two ice cream vendors: where should they park their store to have more customers? The best choice is actually putting them both in the center: this is known as *Hotelling game*, and it's seen in politics too. USA president candidates are actually centrists for this reason. 

### Partnership game

Suppose you've got 2 students working on a project, getting the same grade. Each student must decide how much effort to put into the project: the reward is the sum of these, and a sinergy term capturing the added value generated by working together $r = 4(s_1+s_2+bs_1s_2)$. The cost is $s_i^2$, increasing quadratically. 

### Selfish routing in congestion games

Suppose you have two cities, starting and termination point, and there are two roads. The cost/utility increases with the number of people that choose a road, while for the other it is constant at $1$. Everyone will choose the second hoping for a time that's less than an hour. This is linked to the **price of anarchy**: if someone told you which road to use, the average will be 3/4. The liberty of choosing makes the average 4/3.

## Game theory

Game theory studies **multiagent decision problems**, i.e. problems in which independent decision-makers interact.

Agents have effect on others through their **utility.** Agents have **preferences**: they prefer some outcome over another, and this is encoded in their utility function. 

There are *simultaneous games* in which players take a choice without knowing what the other player did, and *sequential games*.

For the first, we introduce the idea of *payoff matrix*: we have two players, each having two actions, and the payoffs are encoded in a matrix. This only happens for **two agents**.

If you wanted to give a forma definition of a **normal-form game**, we have a tuple $(N,A,u)$, where $N$ is a set of players, $A$ is a set of actions/strategies $A=A_{1} \times A_{2} \times \ldots \times A_{n}$ and an **action profile** will just be the set of actions taken by all the agents in a timestep. Finally, the **utility function** (or payoff) matches each action profile with a given utility: $\mathrm{u}: A \longrightarrow \mathbb{R}^{n}$ where $\mathbf{u}=\left(u_{1}, u_{2}, \ldots, u_{n}\right)$ are the utilities for each agent. 

Von Neumann and Morgenstern proved that utility (and **preference**) is transitive (if you prefer A to B and B to C, you prefer A to C) and that the utility function in non-deterministic cases is just the weighted sum of the outcomes by the probabilities: $u\left(\left\{\left(o_{1}: p_{1}\right),\left(o_{2}: p_{2}\right), \ldots,\left(o_{n}: p_{n}\right)\right\}\right)=\sum_{i=1}^{n} p_{i} u\left(o_{i}\right)$

If we now consider Hotelling's game, we have two players and a **continuous action space**: each player can choose any position between 0 and 1.

A player's **strategy** is the algorithm that determines the action for any stage of the game. A **pure strategy** means **you take a single action, and you play it**. There are more complicated strategies, called **mixed strategies**: here, we don't specify the particular strategy, but a **probability distribution** of the actions. Why would we do that? If you were playing *matching pennies*, you could be outsmarted by your opponent, who would recognize that you always play in a given way. In this case, the expected utility varies: we no longer have a *pure strategy* having utility equal to $u_{i}\left(a_{i}, a_{-i}\right)$, but rather the sum over the possible outcomes $E U_{i}\left(s_{i}, s_{j}\right)=\sum_{k=1}^{n} \sum_{\ell=1}^{m} u_{i}\left(a_{i k}, a_{j \ell}\right) p_{i k} p_{j \ell}$

We're looking at the POV of a single, self-interested agent. What strategy should he adopt? It obviously depends on the actions of the other player too. 

**Pareto optimality**, when dealing with **multiple objectives**, means that no *strategy dominates* the strategy. It is a **solution property**, and it means that a joint action/strategy if no other joint action/strategy profile **Pareto dominates it**. With **Pareto dominance**, we mean that $u_{i}\left(\mathrm{a}^{\prime}\right) \geq u_{i}(\mathrm{a})$ for all agents $i$ and $u_{j}\left(\mathrm{a}^{\prime}\right)>u_{j}(\mathrm{a})$ for some $j$. To get the Pareto optimal points, you draw lines from all the points to get which points are dominated by it: if a point lies in this rectangle, it is dominated. When points do not appear in any rectangle, they are Pareto optimal.

The **best response** from an agent $i$'s point of view, supposing we know the other's agent strategy $s_{-i}$ is defined as the strategy such that $\forall s_{i} \in S_{i}: \quad u_{i}\left(s_{i}^{*}, s_{-i}\right) \geq u_{i}\left(s_{i}, s_{-i}\right)$, meaning that the utility we get with that strategy and the opposer's is higher than any other possible strategy. The problem is, we **don't know the other agent's strategy**! Note that the Best Response is **not necessarily unique!** Let's say we had a strategy composed of two actions: a different mixture between them would mean two different strategies but the same utility.

To find Best Responses, you just check the maximum of every column for the first number, and the maximum of every row for the second.

In Stag Hunt, if I know the other player is going to go for the Stag, my best response is not shooting. 

We can now look at **continuous space games**: we have two players that need to collaborate on a homework assignment. The amount of work they put in is seen as $x,y$. The utility they get is the profit minus the cost: the cost of working one hour does not increase linearly but quadratically. To get the best response in this game, suppose we'd have an input $x$ for player 1, what would the input $y$ of player 2 to maximize the latter utility $u_2$?

To find the maximum utility for a given $x$, we can look at the partial derivative of $u_2$ with respect to $y$: $\frac{\partial u_{2}}{\partial y}=2(1+b x)-2 y=0$ and to find the maximum we equate it to 0. We solve it for y and obtain an optimal response, given $x$, would be  $y^{*} \equiv B R_{2}(x)=1+b x$. If $x$ is specified, we can get the best response! Remember that $b$ is the sinergy factor, which if high, means that the two players work good together. 

Let's now consider two strategies for player $i$, $s_i$ and $s_i'$, and the set $S_{-i}$ of all the strategy profile for the **other players**. We know that $s_i$ **strictly dominates** $s_i'$ if the utility we get from that is higher than the one gotten with $s_i'$ for every strategy in $S_{-i}$, meaning that **no matter what the other agents do, this is always the best strategy**. We can introduce the **weak dominance** as $u_{i}\left(s_{i}, s_{-i}\right) \geq u_{i}\left(s_{i}^{\prime}, s_{-i}\right) \quad \forall s_{-i} \in S_{-i}$ and $u_{i}\left(s_{i}, s_{j}\right)>u_{i}\left(s_{i}^{\prime}, s_{j}\right)$ for at least one $s_j \in S_{-i}$. This means that a strategy weakly dominates another if it is **better or equal when played against all the other players' strategies**, and **better for at least one**.

A strategy is **dominated** if it is at least dominated by one strategy. A **strictly dominated** will never be the best response, and if we've got a strictly dominating strategy we don't have to worry about what the other opponents are going to do: **we're still doing our best**. 

Taking the example of the prisoner's dilemma, we know that *Quiet* is a strictly dominated strategy, because, supposing you stay quiet, you get a -1 if the other one gets quite, but you get 0 if you confess (if the other guy is quiet). If the other confesses, quiet gives you -8. You can eliminate quiet from the strategy as it is strictly dominated. If both comply to this, they will both confess and get the worst *social welfare*. 

## Iterated elimination of strictly dominated strategies (IESDS)

IESDS is based on the assumption that all agents are rational (and they know it), they never play strictly dominated actions, hence, strictly dominated actions can be eliminated. We can get rid of the third column, and notice that middle dominates down. We eliminate down, and we see that the best solution is middle/centre. If you've got rational agents, they will simplify the game and arrive at this solution. 

Watch out for *weakly dominated strategies*: if you eliminate those, you might be getting rid of some solutions you actually want to keep. 

## Minimax and maximin

This allows us to select strategies with specific guarantees/properties. The most natural interpretation is in *zero-sum games*: you either win, or lose, if you win, the other loses. In *general sum games* you have to think of the opponent as **malicious**, wanting to cause the maximum damage to you. 

**Nash is a generalisation of this** which makes sense in general sum games. 

Remember that we're looking from the POV of player $i$, we're always going to look at his utility and maximize it, therefore for $i$ strategies.

Let's first have a look at minimax value. Player $i$ knows that player $j$ is vindictive and malicious, but player i has spy in player j camp who informs him of j's action ($s_j$). Player $i$ will then play his **best response** yielding utility $B R_{i}\left(s_{j}\right) \longrightarrow \max _{s_{i}} u_{i}\left(s_{i}, s_{j}\right)$.

Basically, we know that $i$ will always choose the best action he has available (i.e. the row having highest value), so $j$ will pickthe column that yields the minimum for that, i.e. the minimum of the maximums. This is a **punishing** strategy, as $j$ knows that $i$ will always play his best move. 

$j$ knows that $i$ will always pick the maximum of what he has available, so to punish him, he picks the strategy that will yield the lowest maximum.

**J PUNISHES I**: the player knows that the opponent will always play his best, so he picks the move that minimizes that best.

In order to be malicious, $j$ plays what minimizes the pay-off of $i$'s response!

The total value that $i$ will get from that is actually the minimum of the maximum:$v_{i}^{m i m a}:=\min _{s_{j}} \max _{s_{i}} u_{i}\left(s_{i}, s_{j}\right)$.

The difference between maximin and minimax is that one is a safety strategy, the other is a punishment strategy. You may want to play the latter in repeated games, as you might want to have the opposer understand that he needs to collaborate with you. 

In fact, in MaxiMin, $i$ computes, for each action, the worst possible outcome, then chooses the action that maximises it. Basically, he's trying to secure the bag, maximising the payoff of the worst possible outcome. This is a safety strategy, as we're trying to minimise the gravity of bad things happening. We have a guarantee of payoff, **irrespective** of the actions taken by the other agents. 

In maximin, the player always chooses the safest option, being the less risky one. In minimax, the player is more aggressive and wants to punish the other: to do so, it picks the move that minimizes the other's gain. 

Note that by splitting simultaneous games into random sequential games, it makes sense that minimax and maximin agree: **Minimax theorem**.

The **regret** states how better we could have gone choosing a different alternative than what was choosen using minimax. To perform regret minimisation for the row player, we compute max(row)-min(row) and pick the row having the minimum value.

## Nash equilibrium

This is a generalization for minimax/maximin in non-zero-sum games. This was introduced by Nash in 1950, and it's not a solution, just a solution **condition**. This is formally a strategy $s_i^*$ for player $i$ when they're all playing the **best response to one another**. For every other strategy they could play, the (strict) Nash equilibrium strategy is (strictly) better. This is a **no regret/self-enforcing** strategy, in which no player has the incentive to unilaterally deviate. To compute it, we **find the mutual best responses**, and we check whether there's a cell that has best responses for both players. 

We know, due to the Nash theorem, that a **finite strategic game** has at **least one Nash equilibtrium**.

A finite game could have one or multiple pure-strategy NE. If there's just one, it's easy to decide. If there are multiple, we still need to pick one. *Schelling's focal points* help us in picking the one that is the *most natural*. 

If there's a pure NE we find it looking at the table and finding *joint best responses.* 

Let's though have a look at a *mixed NE*. Think of the *matching pennies example* (you either have to pick heads or tails, if both choose the same, the row player gets +1, the column player gets -1, while if you fail to coordinate the row player gets -1 and the column +1). If you try to see whether there are pure NE, you'll find out that there aren't. Because of Nash's theorem, we know that there has to be a *mixed strategy equilibrium*. If you were to play this, it would actually be smart to randomize. If there's a slight discrepancy in the choices, the opponent may notice that you for example pick 51% heads and 49% tails. Intuitively, the equilibrium in this game would be playing both the choices with a probability of 50%. If you play that, the utility that you get will be the average of all the possible outcomes. You can do that for all the utilities and get 
$$
u_{1}\left(s_{1}, s_{2}\right)=\frac{1}{4} u_{1}(H, H)+\frac{1}{4} u_{1}(T, H)+\frac{1}{4} u_{1}(H, T)+\frac{1}{4} u_{1}(T, T)=0
$$

We can compute this by going back to the utility table, assuming that the row player plays heads with probability $p$ and tails with probability $1-p$. Similarly, column plays heads with $q$ and tails with $1-q$. 

What is the expected outcome of column if the second player plays heads? It is $+1$ with probability $p$ (probability that 1 will play $H$), and $-1$ with probability $1-$. p
$$
u_2(p,H) = -1 p + 1(1-p) = 1-2p \\
u_2(p,T) = p + (-1)(1-p)=2p-1
$$
We can do the same for the first player too. 

It's useful to have a diagram of these utilities. On the $y$ axis you see the utility, on the $x$ the probability that the other player will play $H$. What you see is that the Best Response would be playing basing on the other player's probability of picking $H$, playing T if $q<\frac{1}{2}$ and $H$ if $q>\frac{1}{2}$. If $p$ appears to be exactly $\frac{1}{2}$, you get utility $0$. 

We know $u_1$ is a function of $p$. We want to find the **maximum of it**, so we fix the partial derivative in $p$ to 0. 

Drawing the diagrams (probability on x, utility on y) for mixed nash equilibria is always useful.

We're looking to find the point in which the **expected utilities** are equal. 

## Sperner's Lemma

This is a nice example of a simple theorem with non-trivial results. Let's first explain it with a simple example, called *Rental Harmony*. Imagine that you've got the following problem: there's a shared house with two rooms, the red and green rooms. You and a friend want to rent that house but you have to decide how to split the rent. One room is nicer than the other onem so 50/50 is not an option. Think of this as an interval between 0 and 1, and start by dividing that in a number of subintervals, assigning your name to these points alternatively. You start with A, then B, and so on. It means the line itself represents the division of the rent: every point of this line can be represented by a couple of numbers, where 0,1 is the LHS and 1,0 is RHS. Any *convex combination* represents a point in-between. Imagine this is just like a wooden bar, so if you put all the weight on the left, the center of gravity will be on the left. IF you put them both in the center, it will be balanced. Imagine $0,1$ represents that the guy in the green room pays everything, while in $1,0$ red pays everything. Start with the LHS: A can decide which room he wants, and we check the first point, meaning that red is free. So A will definitely pick the red. On the other side of the interval, the player will pick the green room as that one is free. Suppose we nmow shift lightly to the right from the LHS: the red will still be a better choice, as it's almost free. Every time, players alternatively choose. At some point, **one point will be red and one green**: the division it represents is a situation in which A is happy with the red room, and B is happy with the green room. 

Now, imagine that you have a colored triangle, made in such a way that all the N points have a different color. Nodes on outer edges have **two possible colors only**, while the inside is **free**. Sperner's lemma ensures that there will always be a small subtriangle having **3 differently colored endpoints**. **That's the point of rental harmony** (if we had 3 rooms). This point represents a division of the rent such that everyone is happy. 

## Games with no nash equilibrium

Imagine a sealed bid auction, where the highest bid gets the item, while the 2nd highest sets the price. We can construct games **without NE** to make sure that either the state space is not compact, or the utility function is not continuous. 

A *Vickrey auction* is an example of this. This is a *sealed bid auction* for a single item, having a number of players that partake who have to *submit a sealed bid (envelope with their bid)*. The winner has to pay what the 2nd highest bid was. This avoids inflating the bids, pushing people to be honest on bids. *Truth-telling is a weakly dominant strategy*: there's no strategy being better than being honest with the price. Suppose we had a diagram with the bids from 1 to 4. Suppose the bids were public: you can now change your bid, suppose you wanted to partake. The winner will not change his price, as if he turned it down, he would now avoid winning, but if he turned it up, he would avoid obtaining anything better. 

Suppose the second player (the one setting the price) can now change: if he changes it higher, he can win, but he will pay a price that is higher than what he thinks the item is worth. By turning it up (but less than the winner), he will only change the utility of the winner (raising their prices), but we know that *rational players only care about themselves*, so he doesn't want to punish the winner. 

The last example is concerned with the strategic effects. Imagine the following penalty game: you've got a kicker and a goalie, and the kicker has to decide whether to kick L/R and the goalie has to decide whether to jump L/R simultaneously. The payoff table says that if the goal keeper matches the kicker, the rewards are 0,0, while if they mismatch it's 1,-1.

Let's suppose, though, that the kicker is weaker on a side: if the kicker kicks left and the goalie goes right, there's less of a probability that the kicker will score. If the kicker decides to kick on his weak side, the probability of scoring is less than 1. 

In order to find the NE, you can look at the $p$ value that makes the goalie indifferent: if you want to draw the curves, you check what to play for a given value of $p$.

Note that eliminating **weakly dominated strategies** might erase **NE**s! You have to be careful to only eliminate **strictly dominated strategies**. 

*Schelling's focal points* theorem shows that some equilibria are more natural than others.
