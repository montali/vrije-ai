# Exploration vs Exploitation

We can think of agents interacting with an environment, with states, actions and utilities. Now, what happens if the agent has to **learn the utility**? We have to face the **partial knowledge problem**: in order to maximize the utility, we may want to play the action we know to have the best utility. But on the other hand, we don't really know whether there may be better actions we still did not try yet.

That's what we're going to look at now.

In MAS and RL, there are a number of recurring things: agents are in a certain state, take actions, get a reward and move to another state. 

Sequential decision making consists in a sequential game, in which the agent directly interacts with an **unknown environment**. The learning is **unsupervised**: we don't know the best action, we just know the **reward** we get after taking it and try to learn from it. It's an active way of learning.

We're often talking about stochastic variables: randomly determined numerical outcome. The space of outcomes is mapped onto numbers: $X: \Omega($ "outcomes" $) \longrightarrow \mathbb{R}$. The way you can think about stochastic variables, is an infinite column of numbers. 

We can think of a stochastic variable being a graph, where the set of outcomes is on X, the Y is the number we get. If we get back to the column idea, we note that when we get different numbers, the histogram is the same: this stands for **independent identily distributed**. They're IID in the sense that numbers are different, but the histograms are identical. Suppose I've got N copies of this number generator: the average is still a random variable, but the **distribution of this will be different**. No matter what the distributions of the IID variables are, if you take a sufficient number, they will start looking like a **normal distribution**. The expectation of the sample mean will be $\mu$. Note that $E(\overline{X})$ is the sample mean. The variance of the sample mean is actually the variance of the individual ones divided by n: if you increase the sample size, the uncertainty is reduced!

## Kullback-Leibler Divergence

This is a way to measure distances between two probability distributions. For the formal definition (continuous), you look at the log of the ratio of the two, multiplying by $p$. If you used the Monte Carlo approach rather than computing the integral, you could take a sample from the probability $p$, compute the ratio of p to q, take the log and average. The fact that $p$ disappears, is intrinsic in the fact that you're sampling from $p$.

Divergence is always positive. 

## Exploration

A simple strategy to do exploration is called epsilon-greedy: with probability $1-\epsilon$, you're doing the greedy action. With probability $\epsilon$, you're picking any other. 

In softmax, the agent chooses actions according to a Boltzmann distribution.

To get things started, there's something called **optimistic initialization**: this means that your initial value is going to be unrealistically high. Every value will be tried out, as the estimate will go down, and therefore if you do greedy you will pick something else and go down. That ensures that at least everything is tried at least once. 

One strategy to solve this would be either using greedy with this initialization, or use $\epsilon$-greedy. The question is, *can we do better*? If we look at these strategies, what would be the best type of exploration that we can think of? We have to look at the **opportunity cost**. Think of $q_a$ as the quantity of reward you get, with $q^*$ being the optimal, i.e. the reward corresponding to the best *arm* (k-armed bandit). 

We can prove that the **expected total regret** (mean loss at time t) is the gap for when you play a, by the expected number of times you play that given action. 

Having a look at what would happen for e-greedy. The Expected Total Regret is the same. If you play the optimal strategy, then the expected number of times you play that strategy is $t$ minus the times you play a random one (given by epsilon). When you're not playing the optimal strategy (which happens for a fraction of the time), there are k actions in total, one of them is optimal, so we choose one among the $k-1$ randomly. This means that if you plug the system into the expected loss $\mathcal{l}_t$, you see that when you're playing the optimal one the gap is zero (it doesn't contribute to the sum), and in the other cases the gap is non-zero and multiplied by $\frac{\epsilon}{k-1}t$.

What you see is that the **regret grows linearly in time**. 

## UCB

We theoretically know that the toal regret is at least logarithmic in number of steps. We would therefore like to find a method that is logarithmic indeed, as it's the theoretical optimal we can get. This is represented by **UCB**. 

Here, the error bars indicate how sure we are about a mean. Neither epsilon-greedy nor softmax consider the credibility of the optimal arm. Rather than assuming that the optimal is always certain, we concentrate on most uncertain actions!

Suppose you have a probability density, and you wonder *if you draw from a probability distribution, you usually get something around the mean. What is the probability that the drawn value is really far from the mean?* In other words, what is the probability that the difference between X and the expected value is higher than a given $u$? 

Generally, there are weak bounds for that. If you look at sums of independent variables, you have much tighter bounds: you are working with a normal distribution!

For this, we can introduce the *Chebyschev inequality*. Remember that $EX = \int x f(x) dx$. 

Because of this, we get to $P(X>a) \le EX = \int X dP.$

Applying this, we get to the Markov-Chebychev inequality, which tells you that if you look at the standard deviation, the probability is going to decrease inversely proportional to the square.

Remember that the variance is the expectation of the difference between the value and the mean: $\sigma^2 = E(X-\mu)$

As this is a result that is valid for every distribution, the bound we get is not very tight. If you ask yourself the probability that a value will be larger than a given $x$, what you'll see is that the Markov probability is always $\le$ as stated, but the difference is huge from the real probability!

That is an indication that we can do better. Let's say we did an experiment using the standard normal (centered around 0), and you take a value $x$ and check the probability that a draw exceeds $x$. As you increase $x$, the probability goes down. If you use the Central Limit Theorem, the variance will be the original one divided by n, and we get a probability which is **exponentially bounded**!

We are going to use this result, the previous was just an experiment. If you do it more carefully, you can prove these results too. 

We'll use these to formalize the **UCB algorithm**.

What we should do is being more clever about exploration: after some time, we have to reduce the amount of exploration we do. And that's what we do in epsilon-greedy. In UCB, we do that automatically. 

If you get a value from the probability distribution, you could get anything between $c$ and $d$. When you sample from the distribution, the mean of the samples $\overline{X}$, will be close to $\mu$. The probability of this average being further than $u$ from the real mean will depend on the number of random variables $n$ too:

$P(\bar{X} \leq \mu-u) \equiv P(\mu \geq \bar{X}+u) \leq e^{-2 n u^{2} / L^{2}}$

This tells you the probability that they are far away from $\mu$ or $\bar{X}$. Applying this to our problem, we remember that we're interested in the quality of the action reward information. What we know is that we have sampled that a number of times, and we have an estimate! We can now just look at *Hoeffding's inequality*, where $\mu$ is the unkonwn mean, $u$ is just some sort of deviatino. We're saying that the probability that the actual value exceeds the estimation is bounded by the exponent $e^{-2 n u^{2} / L^{2}}$. $t$ is the total number of pulls we've done, while $L$ is the distance between the two furthest samples. 

We can now try to solve the expression for $U$, which is the amount of distance we're *okay with having between the estimate and the real*. In this way, we want to make sure that over time the exceedance probability decreases, i.e. we're more and more certain that the actual value is in this bound. Over time, we'll have this exceedance probability decrease to zero. If we choose the probability to be $log t^{-1}$, we have know it's equal to $-log t$, therefore we change the numerator from $-\log p(t)$ to $\log t$.

We therefore have that the number of pulls for an arm is $N_t(a)$, we insert that and we pick the next action. The next action is the one having this value be the maximum, i.e. the one we expect to reasonably perform best in the future. If we look at the initial graph, the lower point of a bar is $Q_t(a)$, while the highest point of the bar is $Q_t(a) + U_t(a)$.

We're taking the action that maximizes the upper bound. In an experiment, we take $k=10$ (arms), average 100 experiments. With a logarithmic scale, we notice that the loss is logarithmically linked to the regret. 

### MCTS

The UCB value of a node is the average reward that you get in there + the U-value. Then you move to the child, and you do the same, keeping on doing that until you have no more information to compute the value. The snowcap is defined because at some point there's no more information available to compute U values. At that point, you pick one randomly, all the way down until you get the rewards. Once you've got a value, you back that up: you go back along the path, and you add it to the child nodes in the *snowcap*, getting the updated values. You can now start again: the second time you traverse might be a different path. 

If you have a deep tree, you do something different near the root, as compared to what you do deeper. Near the root you have a **tree policy**, while going deeper you **roll-out**. In the top you try to look at the optimal using UCB, while deeper you have random rollout. 

Imagine you start at the root, and you're going to keep track of two numbers: $t$ is the total reward (the reward at the root basically), which we initialize at $0$. If you compute the UCB value for both the children, you get infinite as we don't know anything. We pick the left one and move there, and from there on we do a random roll-out. We arrive at a leaf, and we backup to the snowcap (we don't really care about the path we're taking). We then know that the total value we've seen is now 40, and back it up to the root. Then we start again at the root, and compute the UCB values. The right one is still infinite, so we move there. We do a random rollout and end up in a leaf that has value 20. We back up again (forgetting about the path!), and update the node's value to 20 and push it up to the root (total$=60$ now). We compute the UCB value, where the denominator is the number of visits for the node, the numerator the number of visits for the root. We pick the child having the highest UCB value. We keep on doing this, and at some point we have to make a decision: at this point our available time is finished and we need to pick one. In the root, the best one is left (average value is 40/2 vs. 30/2). We therefore move to the left, forget about the right subtree, and explore this other subtree as we did with the root before. 

